
This is the page of the Machine Learning class of DISS International Master at Lyon 1 University, for the academic year **year 2025/2026**

-----

### Teachers
Lecturers : 
* Rémy Cazabet (remy.cazabet@univ-lyon1.fr)
* Elia Altimani
* Timon Deschamps
* Bruno Yun
* Louis Bagot

**Teaching Assistant**
* Louis Bagot (louis.bagot@univ-lyon1.fr)

### Program, classes, and content

**Room / Schedule** 

For rooms and other details, please refer to the official Week Schedule there: https://adelb.univ-lyon1.fr/

Below is an overview of the courses and practicals.
As the classes are done, I'll add my presentation slides, some notebooks, etc.
This organization can be subject to changes !.


| Date | Time       | Type        | Teachers | Content |
|-----------|-------------|-------------|------------|------------|
| W 10 Sep   | 9h45-13h00     | CM+TP          | Rémy       | ML intro ([Slides](https://cazabetremy.fr/Teaching/DISS/2025/Data%20-%20Introduction.pdf), [Notes](https://cazabetremy.fr/Teaching/data_class/LectureNotes/DataDescription.pdf), [TP](https://cazabetremy.fr/Teaching/data_class/2025/TP/TP_DataDescription.pdf))|
| W 17 Sep   | 9h45-13h00     | CM+TP          | Rémy       | ML unsupervised + articles ([Slides](https://cazabetremy.fr/Teaching/data_class/2025/slides/Clustering.pdf), [Notes], [TP](https://cazabetremy.fr/Teaching/data_class/2025/TP/TP_CLustering.pdf))|
| W 24 Sep   | 9h45-13h00     | CM+TP          | Rémy+Elia       | ML supervised 1 ([Slides](https://cazabetremy.fr/Teaching/data_class/2025/slides/Supervised%20Learning.pdf),[TP](https://cazabetremy.fr/Teaching/data_class/2025/TP/TP_ML1.pdf))|
| W 24 Sep   | 14h-17h     | CM+TP          | Elia       | Network 1([Slides](https://cazabetremy.fr/Teaching/data_class/Graphs_centrality.pdf),[TP](https://cazabetremy.fr/Teaching/data_class/TPs/graphsGephi.pdf)) |
| W 01 Oct   | 9h45-13h00     | CM+TP          | Rémy+Elia       | ML supervised 2 ([Slides](https://cazabetremy.fr/Teaching/data_class/2025/slides/Supervised%20Learning%20-%202.pdf),[TP](https://cazabetremy.fr/Teaching/data_class/2025/TP/TP_ML2.pdf))|
| W 01 Oct   | 14h-17h      | CM+TP          | Elia       | Network 2 ([Slides](https://cazabetremy.fr/Teaching/data_class/Graphs_ML.pdf),[TP](https://cazabetremy.fr/Teaching/data_class/TPs/graphsPython.pdf))|
| W 08 Oct   | 9h45-13h00     | CM+TP          | Rémy+Elia       | Articles presentation |
| W 08 Oct   | 14h-17h      | CM+TP          | Elia       | Articles presentation |
| W 15 Oct   | 9h45-13h00     | CM          | Timon       | CM: https://temporaldifference.com/slides_deep.pdf - 
exercices: https://temporaldifference.com/exercises_deep.pdf - 
mini tuto pytorch: https://temporaldifference.com/slides_torch.pdf |
| W 22 Oct   | 9h45-13h00     | CM+TP          | Timon       | Intro deep 2  |
| W 29 Oct   | --     | --         | --      | ... |
| W 05 Nov   | 9h45-13h00     | TP          | Louis       | Intro deep : [Lab](https://cazabetremy.fr/Teaching/DISS/2025/Lab_TAA-Intro_to_Deep_Learning.ipynb) - [Hyperparameter optimization](https://docs.google.com/spreadsheets/d/1HW0EZ44Lr4XbPkiEU8jcIAeyyTy5BdSuPU0LxmKL8Sw/edit?usp=sharing) |
| W 12 Nov   | 9h45-13h00     | TP          | Louis       | [Lab2](https://cazabetremy.fr/Teaching/TAA/lab2.pdf) |
| W 19 Nov   | 9h45-13h00     | CM+TP          | Bruno       | [ML](https://bruno-yun.notion.site/INF2494M-Applying-Machine-Learning-in-Real-Life-e10f9c37004c4c09b72c6a47d7a95a68?source=copy_link) |
| W 19 Nov   | 14h-17h      | CM+TP          | Bruno       | [LLM](https://bruno-yun.notion.site/INF2511M-Theory-and-Practical-Applications-of-Large-Language-Models-570124290ae1402ab94b567bfb9b7a08?source=copy_link) |
| W 26 Nov   | 9h45-13h00     | CM+TP          | Bruno       | [LLM](https://bruno-yun.notion.site/INF2511M-Theory-and-Practical-Applications-of-Large-Language-Models-570124290ae1402ab94b567bfb9b7a08?source=copy_link) |
| W 26 Nov   | 14h-17h     | CM+TP          | Bruno       | [LLM](https://bruno-yun.notion.site/INF2511M-Theory-and-Practical-Applications-of-Large-Language-Models-570124290ae1402ab94b567bfb9b7a08?source=copy_link) |
| W 03 Dec   | 9h45-13h00     | CM+TP          | Louis       | DL Paper Presentations |
| W 03 Dec   | 14h-17h     | CM+TP          | Louis       | DL Paper Presentations |
| W 10 Dec   | 9h45-13h00     | CM+TP          | Louis       | Deep Learning advanced |
| W 17 Dec   | 9h45-13h00     | EXAM          |        |  |

-----
## DATA

### Data Description
* [Caffeine Dataset](https://cazabetremy.fr/Teaching/data_class/Datasets/coffee_effects.csv) (Synthetic)
* [Car Dataset](https://cazabetremy.fr/Teaching/data_class/Datasets/cars_synthetic.csv) (Synthetic)

### Clustering

* [Fruits](https://cazabetremy.fr/Teaching/data_class/Datasets/fruits_all.zip)
* [Car Dataset, CLEAN](https://cazabetremy.fr/Teaching/data_class/Datasets/cars_synth_clean.csv) (Synthetic)

### Machine Learning 1, 2

* [Car Dataset, CLEAN](https://cazabetremy.fr/Teaching/data_class/Datasets/cars_synth_clean.csv)  (Synthetic)

### Network 1

* <a href="https://cazabetremy.fr/Teaching/data_class/2025/data/GOT.graphml.zip">small network GOT</a>
* <a href="https://cazabetremy.fr/Teaching/data_class/2025/data/airportsAndCoord.graphml.zip">Airport Network</a>

-----
## Article presentation: Deep Learning
* Description of articles: https://docs.google.com/document/d/1ACqxt8I_WolDDLe5IP1A8uYgGQL0OozAMfurhMovkuQ/edit?usp=sharing
* Article assignments: https://cazabetremy.fr/Teaching/DISS/2025/assign_groups.pdf
* Article reference graph: https://cazabetremy.fr/Teaching/DISS/2025/graph.pdf


## Article presentation : classic ML

### Notes on the article presentation

- **Presentation duration: 25 minutes: 15 minutes presentation, 10 minutes questions. (respecting it is part of the grade)**
- **Select Key Information:** Focus on the most important points of your article. Don't include everything; choose what's most relevant to your audience.
- **Charts and Tables:** When presenting a chart or table, ensure you:
  - Highlight the main takeaways.
  - Direct the audience's attention to significant data points.
  - Provide a brief explanation to set the context.
- **Questions:** Anticipate and address the most significant or likely question your audience might have about your article.
- **Keep Slides Clear:**  
  - Avoid using complete sentences; use bullet points or short phrases instead.  
  - Ensure slides are readable and not crowded with text.
- **Number your slides:** This allows questions to be asked more easily.
- **Presentation Length:** Aim for approximately 15 slides for a 15-minutes presentation. This keeps your content paced well and prevents rushing.
- **Quality over Quantity:** Focus on presenting high-quality, relevant information rather than trying to include everything.
- **Address Uncertainties:** If there's something in the article you don't understand, be transparent about it. This can lead to productive discussions.
- **Originality:**  
  - Avoid copying entire slides from online sources; always create your own content.  
  - Use your own figures, diagrams, and visuals whenever possible. Else, cite the source.
- **Storytelling:** Frame your presentation as a narrative or story. This helps engage your audience and make complex information more accessible.
- **Pacing:** Speak clearly and at a moderate pace. Speaking too quickly can make it difficult for your audience to follow along.
- **Reading:** Do not read! Neither the slides nor a written text. You can use notes if it helps, but only for quick glances.
- **Relevance of Plots:** Only include plots or graphs if there's sufficient time to explain them. If the audience can't understand a plot's relevance, it's best to leave it out.

---

## Proposed articles

- Karczmarek, P., Kiersztyn, A., Pedrycz, W., & Al, E. (2020). *K-Means-based isolation forest*. Knowledge-based systems, 195, 105659.
- Carreira-Perpinán, M. A. (2015). *A review of mean-shift algorithms for clustering*. arXiv preprint arXiv:1503.00687.
- Ankerst, M., Breunig, M. M., Kriegel, H. P., & Sander, J. (1999). *OPTICS: Ordering points to identify the clustering structure*. ACM Sigmod record, 28(2), 49-60.
- Bandyapadhyay, S., Fomin, F. V., Golovach, P. A., Lochet, W., Purohit, N., & Simonov, K. (2023). *How to find a good explanation for clustering?*. Artificial Intelligence, 322, 103948.
- He, X., Zhao, K., & Chu, X. (2021). *AutoML: A survey of the state-of-the-art*. Knowledge-Based Systems, 212, 106622.
- Ran, X., Zhou, X., Lei, M., Tepsan, W., & Deng, W. (2021). *A novel k-means clustering algorithm with a noise algorithm for capturing urban hotspots*. Applied Sciences, 11(23), 11202.
- Zafar, M. B., Valera, I., Rogriguez, M. G., & Gummadi, K. P. (2017, April). *Fairness constraints: Mechanisms for fair classification*. In Artificial intelligence and statistics (pp. 962-970). PMLR.
- Arthur, D., & Vassilvitskii, S. (2007). *k-means++: The advantages of careful seeding.* Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 1027–1035.  
- Frey, B. J., & Dueck, D. (2007). *Clustering by passing messages between data points.* *Science*, 315(5814), 972–976. (Affinity Propagation)  
- Comaniciu, D., & Meer, P. (2002). *Mean shift: A robust approach toward feature space analysis.* *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 24(5), 603–619.  
- Ertöz, L., Steinbach, M., & Kumar, V. (2003). *Finding clusters of different sizes, shapes, and densities.* Proceedings of the 2003 SIAM International Conference on Data Mining (SDM), 47–58. (Shared Nearest Neighbor clustering)  
- Campello, R. J. G. B., Moulavi, D., & Sander, J. (2013). *Density-Based Clustering Based on Hierarchical Density Estimates.* Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), 160–172. (HDBSCAN)  
- Breunig, M. M., Kriegel, H.-P., Ng, R. T., & Sander, J. (2000). *LOF: Identifying density-based local outliers.* *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, 93–104.  
- Zhang, T., Ramakrishnan, R., & Livny, M. (1996). *BIRCH: An efficient data clustering method for very large databases.* *Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data*, 103–114.  
- Ng, A. Y., Jordan, M. I., & Weiss, Y. (2002). *On spectral clustering: Analysis and an algorithm.* *Advances in Neural Information Processing Systems (NeurIPS)*, 849–856.  
- Liben-Nowell, D., & Kleinberg, J. (2007). *The link-prediction problem for social networks.* *Journal of the American Society for Information Science and Technology (JASIST)*, 58(7), 1019–1031.  
- Kleinberg, J. M. (1999). *Authoritative sources in a hyperlinked environment.* *Journal of the ACM*, 46(5), 604–632. (HITS)  
- Watts, D. J., & Strogatz, S. H. (1998). *Collective dynamics of ‘small-world’ networks.* *Nature*, 393, 440–442.  
- Barabási, A.-L., & Albert, R. (1999). *Emergence of scaling in random networks.* *Science*, 286(5439), 509–512.  
- Milo, R., Shen-Orr, S., Itzkovitz, S., Kashtan, N., Chklovskii, D., & Alon, U. (2002). *Network motifs: Simple building blocks of complex networks.* *Science*, 298(5594), 824–827.  
- Perozzi, B., Al-Rfou, R., & Skiena, S. (2014). *DeepWalk: Online learning of social representations.* *Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)*, 701–710.  


-----
## Exam

Coefficients
* CT - final exam: 50% (all subjects)
  *  Past exams, Remy's part, [Example1](https://cazabetremy.fr/Teaching/DISS/Exam2022.pdf), [Example2](https://cazabetremy.fr/Teaching/DISS/Exam2024.pdf)
  
* Presentation ML classic: 10%
* Project Bruno Yun: 20%
* Présentation Deep: 20%


#### Final exam
Final exam will take place on December 17, with questions on all parts of the class by all teachers.
